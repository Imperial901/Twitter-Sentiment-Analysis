{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce722714",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "style.use('ggplot')\n",
    "from textblob import TextBlob\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57c70a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"tweets.csv\", encoding=\"ISO-8859-1\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbea8180",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f31c64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['text'])\n",
    "df['text'].fillna('Unknown', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82340a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1c8507",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8634d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df = df.drop(['Unnamed: 0',  'favorited', 'favoriteCount', 'replyToSN',\n",
    "       'created', 'truncated', 'replyToSID', 'id', 'replyToUID',\n",
    "       'statusSource', 'screenName', 'retweetCount', 'isRetweet', 'retweeted',\n",
    "       'longitude', 'latitude'], axis=1)\n",
    "text_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5660cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text_df['text'].iloc[0],'\\n')\n",
    "print(text_df['text'].iloc[1],'\\n')\n",
    "print(text_df['text'].iloc[2],'\\n')\n",
    "print(text_df['text'].iloc[3],'\\n')\n",
    "print(text_df['text'].iloc[4],'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7c9b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_tweet(sen):\n",
    "    '''Cleans sentence data, handling NaN, and removing extra whitespaces.'''\n",
    "    \n",
    "    # Check for NaN values\n",
    "    if pd.isna(sen):\n",
    "        return \"\"\n",
    "\n",
    "    # Convert to lowercase\n",
    "    sentence = sen.lower()\n",
    "    sentence = re.sub(r\"https\\S+|www\\S+https\\S+\", '',sentence, flags=re.MULTILINE)\n",
    "    sentence = re.sub(r'\\@w+|\\#','',sentence)\n",
    "    sentence = re.sub(r'[^\\w\\s]','',sentence)\n",
    "    # Remove RT\n",
    "    sentence = re.sub(r'^rt\\s+', '', sentence)\n",
    "\n",
    "    # Remove special characters, URLs, and usernames\n",
    "    sentence = re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", sentence)\n",
    "\n",
    "    # Remove single characters\n",
    "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
    "\n",
    "    # Remove multiple spaces\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "\n",
    "    # Strip leading and trailing whitespace\n",
    "    sentence = sentence.strip()\n",
    "    sentence_tokens = word_tokenize(sentence)\n",
    "    filtered_text = [w for w in sentence_tokens if not w in stop_words]\n",
    "    return \" \".join(filtered_text)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92870221",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_tweets = []\n",
    "\n",
    "for tweet in text_df['text']:\n",
    "  cleaned_tweet = preprocess_tweet(tweet)\n",
    "  cleaned_tweets.append(cleaned_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbae011f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "def stemming(data):\n",
    "    text = [stemmer.stem(word) for word in data]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37e5da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df['cleaned'] = pd.DataFrame(cleaned_tweets)\n",
    "text_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc579745",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df['cleaned'].fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239a1392",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df['cleaned'] = text_df['cleaned'].apply(lambda x: stemming(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9def569b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79145d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Assuming you have a DataFrame called text_df with a 'cleaned' column\n",
    "# Convert the 'cleaned' column to strings\n",
    "text_df['cleaned'] = text_df['cleaned'].astype(str)\n",
    "\n",
    "# Apply sentiment analysis using TextBlob\n",
    "text_df[['polarity', 'subjectivity']] = text_df['cleaned'].apply(lambda Text: pd.Series(TextBlob(Text).sentiment))\n",
    "\n",
    "# Perform sentiment analysis using VADER\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "text_df['vader_score'] = text_df['cleaned'].apply(lambda text: analyzer.polarity_scores(text)['compound'])\n",
    "\n",
    "for index, row in text_df['cleaned'].iteritems():\n",
    "    score = SentimentIntensityAnalyzer().polarity_scores(row)\n",
    "    neg = score['neg']\n",
    "    neu = score['neu']\n",
    "    pos = score['pos']\n",
    "    comp = score['compound']\n",
    "    if comp <= -0.05:\n",
    "        text_df.loc[index, 'sentiment'] = \"negative\"\n",
    "    elif comp >= 0.05:\n",
    "        text_df.loc[index, 'sentiment'] = \"positive\"\n",
    "    else:\n",
    "        text_df.loc[index, 'sentiment'] = \"neutral\"\n",
    "    text_df.loc[index, 'neg'] = neg\n",
    "    text_df.loc[index, 'neu'] = neu\n",
    "    text_df.loc[index, 'pos'] = pos\n",
    "    text_df.loc[index, 'compound'] = comp\n",
    "\n",
    "text_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e421621f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5,5))\n",
    "sns.countplot(x='sentiment', data = text_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538e4ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(7,7))\n",
    "colors = (\"yellowgreen\", \"gold\", \"red\")\n",
    "wp = {'linewidth':2, 'edgecolor':\"black\"}\n",
    "tags = text_df['sentiment'].value_counts()\n",
    "explode = (0.1,0.1,0.1)\n",
    "tags.plot(kind='pie', autopct='%1.1f%%', shadow=True, colors = colors,\n",
    "         startangle=90, wedgeprops = wp, explode = explode, label='')\n",
    "plt.title('Distribution of sentiments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54dbdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "neu_tweets = text_df[text_df.sentiment == 'neutral']\n",
    "neu_tweets = neu_tweets.sort_values(['polarity'], ascending= False)\n",
    "neu_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca49ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ' '.join([word for word in neu_tweets['cleaned']])\n",
    "plt.figure(figsize=(20,15), facecolor='None')\n",
    "wordcloud = WordCloud(max_words=500, width=1600, height=800).generate(text)\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.title('Most frequent words in neutral tweets', fontsize=19)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6eb1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tweets = text_df[text_df.sentiment == 'positive']\n",
    "pos_tweets = pos_tweets.sort_values(['polarity'], ascending= False)\n",
    "pos_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e229ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tweets = pos_tweets.dropna(subset=['cleaned'])\n",
    "text = ' '.join([word for word in pos_tweets['cleaned']])\n",
    "plt.figure(figsize=(20,15), facecolor='None')\n",
    "wordcloud = WordCloud(max_words=500, width=1600, height=800).generate(text)\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.title('Most frequent words in positive tweets', fontsize=19)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a45a8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_tweets = text_df[text_df.sentiment == 'negative']\n",
    "neg_tweets = neg_tweets.sort_values(['polarity'], ascending= False)\n",
    "neg_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2524959",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ' '.join([word for word in neg_tweets['cleaned']])\n",
    "plt.figure(figsize=(20,15), facecolor='None')\n",
    "wordcloud = WordCloud(max_words=500, width=1600, height=800).generate(text)\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.title('Most frequent words in negative tweets', fontsize=19)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40e3741",
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(ngram_range=(1,2)).fit(text_df['cleaned'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108f3000",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = vect.get_feature_names_out()\n",
    "print(\"Number of features: {}\\n\".format(len(feature_names)))\n",
    "print(\"First 20 features:\\n {}\".format(feature_names[:20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57dc378",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = text_df['cleaned']\n",
    "Y = text_df['sentiment']\n",
    "X = vect.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22718c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e14878a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Size of x_train:\", (x_train.shape))\n",
    "print(\"Size of y_train:\", (y_train.shape))\n",
    "print(\"Size of x_test:\", (x_test.shape))\n",
    "print(\"Size of y_test:\", (y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a295a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a263748d",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()\n",
    "logreg.fit(x_train, y_train)\n",
    "logreg_pred = logreg.predict(x_test)\n",
    "logreg_acc = accuracy_score(logreg_pred, y_test)\n",
    "print(\"Test accuracy: {:.2f}%\".format(logreg_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff0775b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, logreg_pred))\n",
    "print(\"\\n\")\n",
    "print(classification_report(y_test, logreg_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee3148e",
   "metadata": {},
   "outputs": [],
   "source": [
    "style.use('classic')\n",
    "cm = confusion_matrix(y_test, logreg_pred, labels=logreg.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix = cm, display_labels=logreg.classes_)\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0d4218",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df34b2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid={'C':[0.001, 0.01, 0.1, 1, 10]}\n",
    "\n",
    "grid = GridSearchCV(LogisticRegression(), param_grid)\n",
    "grid.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35040b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best parameters:\", grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ce2f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = grid.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a4c3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_acc = accuracy_score(y_pred, y_test)\n",
    "print(\"Test accuracy: {:.2f}%\".format(logreg_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76a122e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\\n\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee04e43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2338e1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "SVCmodel = LinearSVC()\n",
    "SVCmodel.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240bb6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_pred = SVCmodel.predict(x_test)\n",
    "svc_acc = accuracy_score(svc_pred, y_test)\n",
    "print(\"test accuracy: {:.2f}%\".format(svc_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17169de",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, svc_pred))\n",
    "print(\"\\n\")\n",
    "print(classification_report(y_test, svc_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c411aa17",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = {\n",
    "    'C':[0.01, 0.1, 1, 10],\n",
    "    'kernel':[\"linear\",\"poly\",\"rbf\",\"sigmoid\"],\n",
    "    'degree':[1,3,5,7],\n",
    "    'gamma':[0.01,1]\n",
    "}\n",
    "grid = GridSearchCV(SVCmodel, param_grid)\n",
    "grid.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe46007",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best parameter:\", grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8405b4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = grid.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130f0434",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_acc = accuracy_score(y_pred, y_test)\n",
    "print(\"Test accuracy: {:.2f}%\".format(logreg_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005e9fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\\n\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212a692d",
   "metadata": {},
   "source": [
    "### experimental performanes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cd5dc0",
   "metadata": {},
   "source": [
    "#### tokenization filtration and script validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca206fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "def preprocess_tweet(sen):\n",
    "    '''Cleans sentence data, handling NaN, and removing extra whitespaces.'''\n",
    "    \n",
    "    # Check for NaN values\n",
    "    if pd.isna(sen):\n",
    "        return \"\"\n",
    "\n",
    "    # Convert to lowercase\n",
    "    sentence = sen.lower()\n",
    "    sentence = re.sub(r\"https\\S+|www\\S+https\\S+\", '',sentence, flags=re.MULTILINE)\n",
    "    sentence = re.sub(r'\\@w+|\\#','',sentence)\n",
    "    sentence = re.sub(r'[^\\w\\s]','',sentence)\n",
    "    # Remove RT\n",
    "    sentence = re.sub(r'^rt\\s+', '', sentence)\n",
    "\n",
    "    # Remove special characters, URLs, and usernames\n",
    "    sentence = re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", sentence)\n",
    "\n",
    "    # Remove single characters\n",
    "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
    "\n",
    "    # Remove multiple spaces\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "\n",
    "    # Strip leading and trailing whitespace\n",
    "    sentence = sentence.strip()\n",
    "    sentence = word_tokenize(sentence)\n",
    "    return sentence\n",
    "   \n",
    "    \n",
    "a=preprocess_tweet(text)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce947506",
   "metadata": {},
   "source": [
    "#### stopwords removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdecdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "sentence_tokens = word_tokenize(text)\n",
    "filtered_text = [w for w in sentence_tokens if not w in stop_words]\n",
    "cleaned_text = \" \".join(filtered_text)\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f43054",
   "metadata": {},
   "source": [
    "#### stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3990589",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "def stemming(data):\n",
    "    text = [stemmer.stem(word) for word in data]\n",
    "    return data\n",
    "\n",
    "s= stemming(cleaned_text)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a51e69",
   "metadata": {},
   "source": [
    "#### lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ec2406",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# Download NLTK resources (if not already downloaded)\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d357173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import these modules\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "s= lemmatizer.lemmatize(cleaned_text)\n",
    "print(s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8b003c",
   "metadata": {},
   "source": [
    "#### morphological analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6816c00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download NLTK resources (if not already downloaded)\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876783f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Initialize the WordNet lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Input sentence for analysis\n",
    "sentence =cleaned_text\n",
    "\n",
    "# Tokenize the sentence into words\n",
    "words = sentence_tokens\n",
    "\n",
    "# Perform part-of-speech tagging\n",
    "pos_tags = nltk.pos_tag(words)\n",
    "\n",
    "# Initialize an empty list to store the analysis data\n",
    "analysis_data = []\n",
    "\n",
    "# Loop through each word and perform morphological analysis\n",
    "for word, pos_tag in pos_tags:\n",
    "    lemma = lemmatizer.lemmatize(word)\n",
    "    morph_features = [f\"{tag[0]}={tag[1]}\" for tag in pos_tag.split(\"|\")]\n",
    "    morphological_features = \"|\".join(morph_features)\n",
    "    analysis_data.append(\n",
    "        {\"Token\": word, \"Lemma\": lemma, \"POS\": pos_tag, \"Morphological Features\": morphological_features}\n",
    "    )\n",
    "\n",
    "# Print the formatted output\n",
    "print(\"Token   | Lemma    | POS \")\n",
    "print(\"=\" * 50)\n",
    "for item in analysis_data:\n",
    "    print(f\"{item['Token']:<35} | {item['Lemma']:<35} | {item['POS']:<10} \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36a2110",
   "metadata": {},
   "source": [
    "#### n-gram model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145808d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "# Create and train the model\n",
    "def train_sentiment_model(data):\n",
    "    X_train = text_df['cleaned']\n",
    "    y_train = text_df['sentiment']\n",
    "    \n",
    "    vectorizer = CountVectorizer(ngram_range=(1, 3))\n",
    "    X_train_vec = vectorizer.fit_transform(X_train)\n",
    "    \n",
    "    classifier = MultinomialNB()\n",
    "    classifier.fit(X_train_vec, y_train)\n",
    "    \n",
    "    return vectorizer, classifier\n",
    "\n",
    "# Predict sentiment for new text input\n",
    "def predict_sentiment(text, vectorizer, classifier):\n",
    "    X_test = [text]\n",
    "    X_test_vec = vectorizer.transform(X_test)\n",
    "    \n",
    "    sentiment = classifier.predict(X_test_vec)\n",
    "    \n",
    "    return sentiment[0]\n",
    "\n",
    "# Train the model\n",
    "vectorizer, classifier = train_sentiment_model(df)\n",
    "\n",
    "# Example usage: Predict sentiment for a new text\n",
    "new_text = input('enter the new input:  ')\n",
    "predicted_sentiment = predict_sentiment(new_text, vectorizer, classifier)\n",
    "print(f\"Predicted sentiment: {predicted_sentiment}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8318ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ngrams(sentence, n):\n",
    "    words = sentence.split()\n",
    "    ngrams = [tuple(words[i:i+n]) for i in range(len(words) - (n - 1))]\n",
    "    return ngrams\n",
    "\n",
    "def build_ngram_model(corpus, n):\n",
    "    ngram_model = {}\n",
    "    sentences = corpus.split('. ')\n",
    "    for sentence in sentences:\n",
    "        ngrams = create_ngrams(sentence, n)\n",
    "        for ngram in ngrams:\n",
    "            prefix, suffix = ngram[:-1], ngram[-1]\n",
    "            ngram_model.setdefault(prefix, []).append(suffix)\n",
    "    return ngram_model\n",
    "\n",
    "def predict_next_word(prefix, ngram_model):\n",
    "    possible_next_words = ngram_model.get(prefix, [])\n",
    "    if possible_next_words:\n",
    "        return max(set(possible_next_words), key=possible_next_words.count)\n",
    "    return None\n",
    "\n",
    "n = int(input(\"Enter the value of N: \"))\n",
    "corpus =cleaned_text\n",
    "\n",
    "ngram_model = build_ngram_model(corpus, n)\n",
    "\n",
    "input_prefix = tuple(input(f\"Enter a prefix of length {n-1}: \").split())\n",
    "predicted_word = predict_next_word(input_prefix, ngram_model)\n",
    "\n",
    "if predicted_word:\n",
    "    print(f\"Predicted next word: {predicted_word}\")\n",
    "else:\n",
    "    possible_prefix = max((prefix for prefix in ngram_model if prefix[:-1] == input_prefix[1:]), key=len, default=None)\n",
    "    if possible_prefix:\n",
    "        possible_next_words = ngram_model[possible_prefix]\n",
    "        most_likely_word = max(set(possible_next_words), key=possible_next_words.count)\n",
    "        print(f\"No direct prediction. Predicting the most likely word based on frequency: {most_likely_word}\")\n",
    "    else:\n",
    "        print(\"No prediction available for the given prefix.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed792b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict the sentiment\n",
    "import nltk\n",
    "from nltk import RegexpParser\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = cleaned_text\n",
    "\n",
    "# Tokenize the text into words\n",
    "words = word_tokenize(text)\n",
    "\n",
    "# Perform part-of-speech tagging\n",
    "pos_tags = nltk.pos_tag(words)\n",
    "# Define a grammar for noun phrases (NP)\n",
    "grammar = r\"\"\"\n",
    "    NP: {<DT>?<JJ>*<NN>}  # Chunk sequences of determiners, adjectives, and nouns\n",
    "\"\"\"\n",
    "# Create a chunk parser with the defined grammar\n",
    "chunk_parser = RegexpParser(grammar)\n",
    "# Parse the POS-tagged words to find noun phrases\n",
    "tree = chunk_parser.parse(pos_tags)\n",
    "print(tree)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c41417",
   "metadata": {},
   "source": [
    "#### POS Tagging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a77015",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "text = cleaned_text\n",
    "\n",
    "# Tokenize the text into words\n",
    "sentence_tokens = word_tokenize(text)\n",
    "words = sentence_tokens\n",
    "\n",
    "# Perform POS tagging\n",
    "pos_tags = nltk.pos_tag(words)\n",
    "\n",
    "# Print the POS tags\n",
    "print(\"Word  |  POS Tag\")\n",
    "print(\"-\" * 15)\n",
    "for word, pos_tag in pos_tags:\n",
    "    print(f\"{word:<30} | {pos_tag}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a06938",
   "metadata": {},
   "source": [
    "#### chunking of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d0c603",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import RegexpParser\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "text = cleaned_text\n",
    "\n",
    "# Tokenize the text into words\n",
    "sentence_tokens = word_tokenize(text)\n",
    "words= sentence_tokens\n",
    "\n",
    "# Perform part-of-speech tagging\n",
    "pos_tags = nltk.pos_tag(words)\n",
    "\n",
    "# Define a grammar for noun phrases (NP)\n",
    "grammar = r\"\"\"\n",
    "    NP: {<DT>?<JJ>*<NN>}  # Chunk sequences of determiners, adjectives, and nouns\n",
    "\"\"\"\n",
    "# Create a chunk parser with the defined grammar\n",
    "chunk_parser = RegexpParser(grammar)\n",
    "\n",
    "# Parse the POS-tagged words to find noun phrases\n",
    "tree = chunk_parser.parse(pos_tags)\n",
    "\n",
    "print(tree)# can use tree.pretty_print() method\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf01523f",
   "metadata": {},
   "source": [
    "#### NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b4e64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm/en_core_web_sm-3.6.0/\")\n",
    "doc = nlp(cleaned_text)\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273be787",
   "metadata": {},
   "source": [
    "#### text similarity recognizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5d8120",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm/en_core_web_sm-3.6.0/\")\n",
    "\n",
    "document1 = \"I find joy in exploring the wonders of natural language processing.\"\n",
    "document2 = \"NLP is a fascinating discipline that intersects with artificial intelligence.\"\n",
    "document3 = \"Machine learning, an integral part of AI, empowers algorithms to decipher patterns.\"\n",
    "\n",
    "doc1 = nlp(document1)\n",
    "doc2 = nlp(document2)\n",
    "doc3 = nlp(document3)\n",
    "\n",
    "similarity_doc1_doc2 = doc1.similarity(doc2)\n",
    "similarity_doc1_doc3 = doc1.similarity(doc3)\n",
    "similarity_doc2_doc3 = doc2.similarity(doc3)\n",
    "\n",
    "# Print the similarity scores\n",
    "print(\"Similarity between document 1 and document 2:\", similarity_doc1_doc2)\n",
    "print(\"Similarity between document 1 and document 3:\", similarity_doc1_doc3)\n",
    "print(\"Similarity between document 2 and document 3:\", similarity_doc2_doc3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4cfef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in text_df.loc[text_df['sentiment'] == 'positive', 'cleaned']:\n",
    "    print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed725eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "\n",
    "# Load the spaCy language model\n",
    "nlp = spacy.load(\"en_core_web_sm/en_core_web_sm-3.6.0/\")\n",
    "\n",
    "# Assuming you have already created the DataFrame text_df\n",
    "# Replace 'text_df' with your actual DataFrame name\n",
    "\n",
    "# Get cleaned texts for each sentiment\n",
    "positive_texts = text_df.loc[text_df['sentiment'] == 'positive', 'cleaned'].tolist()\n",
    "negative_texts = text_df.loc[text_df['sentiment'] == 'negative', 'cleaned'].tolist()\n",
    "neutral_texts = text_df.loc[text_df['sentiment'] == 'neutral', 'cleaned'].tolist()\n",
    "\n",
    "# Concatenate the texts into single strings\n",
    "document1 = ' '.join(positive_texts)\n",
    "document2 = ' '.join(negative_texts)\n",
    "document3 = ' '.join(neutral_texts)\n",
    "\n",
    "# Process the documents using spaCy\n",
    "doc1 = nlp(document1)\n",
    "doc2 = nlp(document2)\n",
    "doc3 = nlp(document3)\n",
    "\n",
    "# Calculate similarity scores (cosine similarity) between documents\n",
    "similarity_doc1_doc2 = doc1.similarity(doc2)\n",
    "similarity_doc1_doc3 = doc1.similarity(doc3)\n",
    "similarity_doc2_doc3 = doc2.similarity(doc3)\n",
    "\n",
    "# Print the similarity scores\n",
    "print(\"Similarity between document 1 and document 2:\", similarity_doc1_doc2)\n",
    "print(\"Similarity between document 1 and document 3:\", similarity_doc1_doc3)\n",
    "print(\"Similarity between document 2 and document 3:\", similarity_doc2_doc3)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
